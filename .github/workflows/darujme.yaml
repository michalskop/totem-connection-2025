name: Download Darujme Data
on:
  schedule:
    - cron: '0 0 * * *'  # Run daily at midnight
  workflow_dispatch:

jobs:
  process-data:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'
          
      - name: Install dependencies
        run: pip install -r ./scripts/requirements.txt
        
      - name: Create temporary directory
        run: mkdir -p ./temp
        
      - name: Download data
        env:
          DARUJME_ORG_ID: ${{ secrets.DARUJME_ORG_ID }}
          DARUJME_API_ID: ${{ secrets.DARUJME_API_ID }}
          DARUJME_API_SECRET: ${{ secrets.DARUJME_API_SECRET }}
          DARUJME_TIMEFRAME: "week"
          DARUJME_OUTPUT_FILE: "./temp/darujme_data.json"
        run: python ./scripts/darujme.py

      # # Example of next step that uses the data
      # - name: Process the data
      #   run: |
      #     python process_data.py --input ./temp/darujme_data.json --output ./temp/results.csv
          
      # Store processed results using v4
      # - name: Store processed results
      #   if: success()
      #   uses: actions/upload-artifact@v4
      #   with:
      #     name: processed-results
      #     path: ./temp/results.csv
      #     retention-days: 1
      #     compression-level: 9  # New in v4: maximum compression
      #     overwrite: true      # New in v4: explicitly handle duplicates
          
      # Clean up sensitive data
      - name: Clean up
        if: always()
        run: |
          rm -rf ./temp